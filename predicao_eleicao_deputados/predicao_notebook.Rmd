---
title: "Predição eleição deputados"
author: "Vinicius Brandão Araujo"
date: 25/11/2018
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
  html_notebook:
    toc: yes
    toc_float: yes
---


```{r setup, include=FALSE}
library(caret)
library(tidyverse)
library(gridExtra)
library(xgboost)
library(grid)
library(DMwR)
```


```{r}
train_data <- read.csv("data/train.csv")
test_data <- read.csv("data/test.csv")
```

# Descrição
Analise desenvolvido para a disciplina de Ciência de Dados Preditiva no período de 2018.2, o principal objetivo desta analise é prever quais candidatos à Câmara de Deputados serão eleitos nas eleições de 2014.

## Há desbalanceamento das classes (isto é, uma classe tem muito mais instâncias que outra)? Em que proporção? Quais efeitos colaterais o desbalanceamento de classes pode causar no classificador? Como você poderia tratar isso?

Como o principal intuito é a predição se um candidato foi eleito ou não, devemos considerar a classe situação e assim observar como está distribuida no conjunto de treinamento.

```{r}
train_data %>% ggplot(aes(situacao)) + geom_bar(aes(y = (..count..)/sum(..count..))) + scale_y_continuous(labels=scales::percent) + ylab("Porcentagem relativa nos dados")
```

Observamos que a situação não eleito está presente em mais de 80% dos dados de treinamento, nesse caso observamos uma desbalanceamento em uma grande proporção, considerando que eleito equivale a menos de 15% dos dados.
Desse modo, pode ser criado um viés na qual o modelo aprenderá a ignorar as classes menos frequentes, levando a um impacto negativo na generalização do modelo e seu desempenho.
O tratamento desses desbalanceamento pode ser dado de duas formas, são elas:

+ Undersampling
+ Oversampling

## Pre-processamento dos dados

Dividimos o conjunto de traino em traino e validação para poder validar o modelo.
```{r}
partition <- createDataPartition(y = train_data$situacao, p=0.75, list=FALSE)
set.seed(9560)
train <- train_data[partition, ]
validacao <- train_data[-partition, ]
```


Iremos "limpar os dados" desconsiderando algumas variaveis na qual não consideramos nescessaria para a geração dos modelos.
```{r}
set.seed(107)
train <- train %>%   
          select(-partido,
                 -uf,-nome,
                 -estado_civil,
                 -ocupacao,-ano,
                 -cargo,-grau,-sexo,
                 -sequencial_candidato)

validacao <- validacao %>%   
          select(-partido,
                 -uf,-nome,
                 -estado_civil,
                 -ocupacao,-ano,
                 -cargo,-grau,-sexo,
                 -sequencial_candidato)
test <- test_data %>%   
          select(-partido,
                 -uf,-nome,
                 -estado_civil,
                 -ocupacao,-ano,
                 -cargo,-grau,-sexo)
```

Levando em consideração o desbalacemaneto existente, iremos utilizar a função SMOTE para o balancemaneto. 

> A função SMOTE faz sobreamostragem de seu evento raro usando bootstrapping e k-neighbor mais próximo para criar sinteticamente observações adicionais daquele evento.

```{r}
set.seed(107)
train <- train %>% SMOTE(situacao ~ .,
                          data = ., 
                          perc.over = 200, 
                          perc.under=200)

train %>%
  group_by(situacao) %>%
  summarise(num = n()) %>%
  ungroup() %>%
  mutate(total = sum(num))
```

### Funções auxiliares

```{r}
F_Measure <- function(expected, predicted, ...) {
resultado<-  data.frame(expected=expected,
             prediction=predicted) %>%
      mutate(tp = ifelse(expected == "eleito" & 
                         prediction == "eleito",1,0),
             fn = ifelse(expected == "eleito" &
                         prediction == "nao_eleito",1,0),
             fp = ifelse(expected == "nao_eleito" &
                         prediction == "eleito",1,0)) 
 resultado <-  resultado  %>%
    summarize(x = sum(tp),
              y = sum(fp),
              z = sum(fn)) %>%
    mutate(recall = x / (x + z),
           precision = x/ (x + y),
           f_measure = 2 * (precision * recall) / (precision + recall))
  resultado$x <- NULL
  resultado$y <- NULL
  resultado$z <- NULL
  return(resultado)
}
```

## Treine: um modelo de KNN, regressão logística, uma árvore de decisão e um modelo de adaboost. Tune esses modelos usando validação cruzada e controle overfitting se necessário, considerando as particularidades de cada modelo. 

Para a geração dos modelos, iremos utilizar á biblioteca caret e assim trainar os modelos usando as funções trainControl e train.

### KNN
```{r}
cross_validation <- trainControl(method = "boot", classProbs = TRUE,savePredictions = "final")
k <- expand.grid(k = seq(from=1, to=50, by=1))
model_knn <- train(situacao ~ ., 
                     data = train, 
                     method = "knn", 
                     tuneGrid = k,
                     preProc = c("center", "scale"),
                     trControl = cross_validation)
plot(model_knn)
model_knn
```

### Regressão Logística
```{r}
rlGrid <- expand.grid(cost = c(200,2,0.02),loss = c("L2","L2_dual","L2_primal"), epsilon = c(0.001,0.01))
cross_validation <- trainControl(method = "boot",classProbs = TRUE,savePredictions = "final")
model_logistic<- train(situacao ~ ., 
                     data = train, 
                     method = "regLogistic",
                     metric = "F1",
                     preProc = c("center", "scale"),
                     trControl = cross_validation)
plot(model_logistic)
model_logistic
```

### Árvore de Decisão

```{r}
cross_validation <- trainControl(method = "boot",classProbs = TRUE,savePredictions = "final")

model_tree<- train(situacao ~ .,
                     metric = "F1",
                     data = train, 
                     method = "rpart",
                     preProc = c("center", "scale"),
                     trControl = cross_validation)
plot(model_tree)
model_tree
```

### Adaboost

```{r}
cross_validation <- trainControl(method = "boot", verboseIter = FALSE,savePredictions = "final")

model_ada<- train(situacao ~ ., 
                     data = train, 
                     method = "ada",
                     preProc = c("center", "scale"),
                     trControl = cross_validation)
plot(model_ada)
model_ada

```



## Reporte precision, recall e f-measure no treino e validação. Há uma grande diferença de desempenho no treino/validação? Como você avalia os resultados? 

### KNN

#### Treino
```{r}
model_knn %$%
  pred %>%
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

#### Validação
```{r}
validacao %>%
  predict(object=model_knn,.) %>%
  F_Measure(validacao$situacao,.)
```

Observamos que no conjunto de validação os valores de recall, precision e f_meansure baixo em relação ao conjunto de treino.

### Regressão Logística

#### Treino
```{r}
model_logistic %$%
  pred %>%
  F_Measure(expected = .$obs,
            predicted = .$pred)
```

#### Validação
```{r}
validacao %>%
  predict(object=model_logistic,.) %>%
  F_Measure(validacao$situacao,.)
```
Observamos que no conjunto de validação os valores de precision e f_meansure baixo em relação ao conjunto de treino.

### Árvore de Decisão

#### Treino
```{r}
model_tree %$%
  pred %>%
  F_Measure(expected = .$obs,
            predicted = .$pred)
```
#### Validação
```{r}
validacao %>%
  predict(object=model_tree,.) %>%
  F_Measure(validacao$situacao,.)
```

Observamos que no conjunto de validação os valores de precision e f_meansure baixo em relação ao conjunto de treino.

### Adaboost

#### Treino
```{r}
model_ada %$%
  pred %>%
  F_Measure(expected = .$obs,
            predicted = .$pred)
```
#### Validação
```{r}
validacao %>%
  predict(object=model_ada,.) %>%
  F_Measure(validacao$situacao,.)
```

Observamos que no conjunto de validação os valores de precision e f_meansure baixo em relação ao conjunto de treino.

### Conclusão
Como observamos todos os modelos os valores de precision e f_mensure é baixo perante ao conjunto de treino.

## Interprete as saídas dos modelos. Quais atributos parecem ser mais importantes de acordo com cada modelo?

### KNN
```{r}
ggplot(varImp(model_knn))
```

Observamos que para o knn os atributos mais importantes são:

+ total_receita
+ total_despesa
+ recursos_de_pessoas_juridicas
+ quantidade_fornecedores
+ quantidade_despesas

### Regressão Logística
```{r}
ggplot(varImp(model_logistic))
```

Observamos que para o regressão logística os atributos mais importantes são:

+ total_receita
+ total_despesa
+ recursos_de_pessoas_juridicas
+ quantidade_fornecedores
+ quantidade_despesas

### Árvore de Decisão
```{r}
ggplot(varImp(model_tree))
```

Observamos que para árvore de decisão os atributos mais importantes são:

+ total_despesa
+ total_receita
+ recursos_de_pessoas_juridicas
+ quantidade_fornecedores
+ quantidade_despesas

### Adaboost
```{r}
ggplot(varImp(model_ada))
```

Observamos que para o adaboost os atributos mais importantes são:

+ total_receita
+ total_despesa
+ recursos_de_pessoas_juridicas
+ quantidade_fornecedores
+ quantidade_despesas

