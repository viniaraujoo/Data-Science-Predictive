verbose = F)
train <- one_hot_encoder(dataSet = train,
encoding = encoding,
drop = TRUE,
verbose = F)
cat("#### Train Shape",
"\n##### Observations: ",nrow(train),
"\n##### Variables: ",ncol(train))
train.rose <- one_hot_encoder(dataSet = train.rose,
encoding = encoding,
drop = TRUE,
verbose = F)
cat("#### Data Shape",
"\n##### Observations: ",nrow(train.rose),
"\n##### Variables: ",ncol(train.rose))
test <- one_hot_encoder(dataSet = test,
encoding = encoding,
drop = TRUE,
verbose = F)
cat("#### Data Shape",
"\n##### Observations: ",nrow(test),
"\n##### Variables: ",ncol(test))
test <- one_hot_encoder(dataSet = test,
encoding = encoding,
drop = TRUE,
verbose = F)
train %>%
nearZeroVar(saveMetrics = TRUE) %>%
tibble::rownames_to_column("variable") %>%
filter(nzv == T) %>%
pull(variable) -> near_zero_vars
train %>%
select(-one_of(near_zero_vars)) -> train
train.rose %>%
select(-one_of(near_zero_vars)) -> train.rose
test %>%
select(-one_of(near_zero_vars)) -> test
near_zero_vars %>%
glimpse()
split_target_predictors <- function(df, target_range) {
df_matrix <- as.matrix(df)
dimnames(df_matrix) <- NULL
x_data <- df_matrix[,-target_range]
y_data <- df_matrix[,target_range]
newData <- list("predictors" = x_data, "target" = y_data)
return(newData)
}
train_df <- train
train <- as.matrix(train)
dimnames(train) <- NULL
x_train <- train[,1:38]
y_train <- train[,39:40]
y_train %>%
head(10)
View(x_train)
View(y_train)
View(train_df)
neuralNetCV <- function(df,
target_range,
model,
k=5,
loss_method = 'binary_crossentropy',
optim_approach = 'adam',
summary_metrics='accuracy',
epochs = 200,
batch_size = 5,
validation_split = 0.2) {
model %>%
compile(
loss = loss_method,
optimizer = optim_approach,
metrics = summary_metrics)
df %>%
mutate(folds = sample(rep_len(1:k, nrow(.)))) -> df
result <-  data.frame("loss"=c(),metrics=c())
for(f in unique(df$folds)){
# split into train/validation
df %>%
filter(folds == f) -> train_df
df %>%
filter(folds != f) -> valid_df
# Remove auxiliary column
train_df %>%
select(-folds) -> train_df
valid_df %>%
select(-folds) -> valid_df
# create matrices
train_df %>%
split_target_predictors(.,target_range = target_range) -> x_train
valid_df %>%
split_target_predictors(.,target_range = target_range) -> x_valid
# extract target and predictors
y_train <- x_train$target
x_train <- x_train$predictors
y_valid <- x_valid$target
x_valid <- x_valid$predictors
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = epochs,
batch_size = batch_size,
validation_split = validation_split)
# Evaluate the model
model %>%
evaluate(x_valid,
y_valid,
batch_size = 128) -> score
score %>%
as.data.frame() -> temporary
result <- rbind(result,temporary)
}
cvsummary <- list("result"=result,
"history"=history,
"model"=model)
return(cvsummary);
}
tuneNeuralNetwork <- function(model,data,paramsGrid,
target_range= 39:40,k=5) {
environment(neuralNetCV) <- environment()
best_accuracy <- 0
best_loss <- 0
best_tune <- NULL
best_history <- NULL
best_model <- NULL
optmizer <- NULL
losses <- c()
acc <- c()
for(i in 1:nrow(paramsGrid)) {
#     for(i in c(1)) {
row <- paramsGrid[i,]
if (row$optim_approach == "sgd")
optmizer <- optimizer_sgd(lr = 0.01)
else if (row$optim_approach == "rmsprop")
optmizer <- optimizer_rmsprop(lr = 0.001, rho = 0.9)
else if (row$optim_approach == "adam")
optmizer <- optimizer_adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)
# apply CV Kfold to particular set of params
neuralNetCV(data,
target_range,
model,
k,
loss_method = paste0(row$loss_method),
optim_approach = optmizer,
summary_metrics= paste0(row$summary_metrics),
epochs = paste0(row$epochs),
batch_size = paste0(row$batch_size),
validation_split = paste0(row$validation_split)) -> cvsummary
cvsummary %$%
result %$%
acc %>%
mean() -> mean_accuracy
cvsummary %$%
result %$%
loss %>%
mean() -> mean_loss
losses <- append(losses,mean_loss)
acc <- append(acc, mean_accuracy)
if(mean_accuracy > best_accuracy) {
best_accuracy <- mean_accuracy
best_history <- cvsummary$history
best_model <- cvsummary$model
best_result <- mean_accuracy
best_loss <- mean_loss
best_tune <- row
}
}
paramsGrid$loss <- losses
paramsGrid$accuracy <- acc
best_tune$accuracy <- best_accuracy
best_tune$loss <- best_loss
result <- list("history"=best_history,
"best_tune"=best_tune,
"iterations"=paramsGrid,
"best_model"=best_model)
return(result)
}
optm <- c("sgd", "rmsprop", "adam")
validation_split <- c(0.2,0.3,0.4)
paramsGrid <- expand.grid(optim_approach=optm,
loss_method = 'binary_crossentropy',
summary_metrics='accuracy',
epochs = 200,
batch_size = 5,
validation_split = validation_split)
paramsGrid
readr::read_csv(here::here('data/train.csv'),
progress = FALSE,
local=readr::locale("br"),
col_types = cols(ano = col_integer(),
sequencial_candidato = col_character(),
quantidade_doacoes = col_integer(),
quantidade_doadores = col_integer(),
total_receita = col_double(),
media_receita = col_double(),
recursos_de_outros_candidatos.comites = col_double(),
recursos_de_pessoas_fisicas = col_double(),
recursos_de_pessoas_juridicas = col_double(),
recursos_proprios = col_double(),
`recursos_de_partido_politico` = col_double(),
quantidade_despesas = col_integer(),
quantidade_fornecedores = col_integer(),
total_despesa = col_double(),
media_despesa = col_double(),
situacao = col_character(),
.default = col_character())) %>%
mutate(sequencial_candidato = as.numeric(sequencial_candidato),
estado_civil = as.factor(estado_civil),
ocupacao = as.factor(ocupacao),
situacao = as.factor(situacao),
partido = as.factor(partido),
cargo = as.factor(cargo),
nome = as.factor(nome),
grau = as.factor(grau),
sexo = as.factor(sexo),
uf = as.factor(uf)) -> data
data %>%
group_by(situacao) %>%
summarise(num = n()) %>%
ungroup() %>%
mutate(total = sum(num),
proportion = num/total)
set.seed(107)
data$id <- 1:nrow(data)
data %>%
dplyr::sample_frac(.8) -> train
cat("#### Train Shape",
"\n##### Observations: ",nrow(train),
"\n##### Variables: ",ncol(train))
dplyr::anti_join(data,
train,
by = 'id') -> test
cat("#### Test Shape",
"\n##### Observations: ",nrow(test),
"\n##### Variables: ",ncol(test))
train <- train %>%
select(-partido,
-uf,-nome,
-estado_civil,
-ocupacao,-ano,
-cargo,-grau,-sexo,
-sequencial_candidato)
test <- test_data %>%
select(-partido,
-uf,-nome,
-estado_civil,
-ocupacao,-ano,
-cargo,-grau,-sexo)
train <- train %>%
select(-partido,
-uf,-nome,
-estado_civil,
-ocupacao,-ano,
-cargo,-grau,-sexo,
-sequencial_candidato)
test <- test %>%
select(-partido,
-uf,-nome,
-estado_civil,
-ocupacao,-ano,
-cargo,-grau,-sexo)
train %>%
dplyr::select_if(.,is.numeric) -> train.numeric
train %>%
dplyr::select_if(.,negate(is.numeric)) -> train.categorical
test %>%
dplyr::select_if(.,is.numeric) -> test.numeric
test %>%
dplyr::select_if(.,negate(is.numeric)) -> test.categorical
train.numeric %>%
preProcess(.,method = c("center","scale")) -> processParams
processParams %>%
predict(.,train.numeric) -> train.numeric
processParams %>%
predict(.,test.numeric) -> test.numeric
processParams
train.numeric %>%
dplyr::bind_cols(train.categorical) -> train
test.numeric %>%
dplyr::bind_cols(test.categorical) -> test
test <- one_hot_encoder(dataSet = test,
encoding = encoding,
drop = TRUE,
verbose = F)
split_target_predictors <- function(df, target_range) {
df_matrix <- as.matrix(df)
dimnames(df_matrix) <- NULL
x_data <- df_matrix[,-target_range]
y_data <- df_matrix[,target_range]
newData <- list("predictors" = x_data, "target" = y_data)
return(newData)
}
train_df <- train
train <- as.matrix(train)
dimnames(train) <- NULL
x_train <- train[,1:38]
View(train)
train_df <- train_df  %>% mutate(situacao = unclass(train_df$situacao))
train <- as.matrix(train_df)
View(train)
dimnames(train) <- NULL
x_train <- train[,1:13]
y_train <- train[,14]
y_train %>%
head(10)
View(x_train)
y_train %>%
head(10)
y_train
dim(train)
x_train <- train[,1:14]
y_train <- train[,15]
y_train %>%
head(10)
neuralNetCV <- function(df,
target_range,
model,
k=5,
loss_method = 'binary_crossentropy',
optim_approach = 'adam',
summary_metrics='accuracy',
epochs = 200,
batch_size = 5,
validation_split = 0.2) {
model %>%
compile(
loss = loss_method,
optimizer = optim_approach,
metrics = summary_metrics)
df %>%
mutate(folds = sample(rep_len(1:k, nrow(.)))) -> df
result <-  data.frame("loss"=c(),metrics=c())
for(f in unique(df$folds)){
# split into train/validation
df %>%
filter(folds == f) -> train_df
df %>%
filter(folds != f) -> valid_df
# Remove auxiliary column
train_df %>%
select(-folds) -> train_df
valid_df %>%
select(-folds) -> valid_df
# create matrices
train_df %>%
split_target_predictors(.,target_range = target_range) -> x_train
valid_df %>%
split_target_predictors(.,target_range = target_range) -> x_valid
# extract target and predictors
y_train <- x_train$target
x_train <- x_train$predictors
y_valid <- x_valid$target
x_valid <- x_valid$predictors
# Train model
history <- model %>% fit(
x_train, y_train,
epochs = epochs,
batch_size = batch_size,
validation_split = validation_split)
# Evaluate the model
model %>%
evaluate(x_valid,
y_valid,
batch_size = 128) -> score
score %>%
as.data.frame() -> temporary
result <- rbind(result,temporary)
}
cvsummary <- list("result"=result,
"history"=history,
"model"=model)
return(cvsummary);
}
tuneNeuralNetwork <- function(model,data,paramsGrid,
target_range= 39:40,k=5) {
environment(neuralNetCV) <- environment()
best_accuracy <- 0
best_loss <- 0
best_tune <- NULL
best_history <- NULL
best_model <- NULL
optmizer <- NULL
losses <- c()
acc <- c()
for(i in 1:nrow(paramsGrid)) {
#     for(i in c(1)) {
row <- paramsGrid[i,]
if (row$optim_approach == "sgd")
optmizer <- optimizer_sgd(lr = 0.01)
else if (row$optim_approach == "rmsprop")
optmizer <- optimizer_rmsprop(lr = 0.001, rho = 0.9)
else if (row$optim_approach == "adam")
optmizer <- optimizer_adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)
# apply CV Kfold to particular set of params
neuralNetCV(data,
target_range,
model,
k,
loss_method = paste0(row$loss_method),
optim_approach = optmizer,
summary_metrics= paste0(row$summary_metrics),
epochs = paste0(row$epochs),
batch_size = paste0(row$batch_size),
validation_split = paste0(row$validation_split)) -> cvsummary
cvsummary %$%
result %$%
acc %>%
mean() -> mean_accuracy
cvsummary %$%
result %$%
loss %>%
mean() -> mean_loss
losses <- append(losses,mean_loss)
acc <- append(acc, mean_accuracy)
if(mean_accuracy > best_accuracy) {
best_accuracy <- mean_accuracy
best_history <- cvsummary$history
best_model <- cvsummary$model
best_result <- mean_accuracy
best_loss <- mean_loss
best_tune <- row
}
}
paramsGrid$loss <- losses
paramsGrid$accuracy <- acc
best_tune$accuracy <- best_accuracy
best_tune$loss <- best_loss
result <- list("history"=best_history,
"best_tune"=best_tune,
"iterations"=paramsGrid,
"best_model"=best_model)
return(result)
}
optm <- c("sgd", "rmsprop", "adam")
validation_split <- c(0.2,0.3,0.4)
paramsGrid <- expand.grid(optim_approach=optm,
loss_method = 'binary_crossentropy',
summary_metrics='accuracy',
epochs = 200,
batch_size = 5,
validation_split = validation_split)
paramsGrid
teste <- neuralNetCV(x_train,y_train,model = model)
# # Initialize a sequential model
model <- keras_model_sequential()
#
# # Add layers to the model
model.simple %>%
layer_dense(units = 8, activation = 'relu', input_shape = c(38)) %>%
layer_dense(units = 2, activation = 'softmax')
#
# # Add layers to the model
model %>%
layer_dense(units = 8, activation = 'relu', input_shape = c(38)) %>%
layer_dense(units = 2, activation = 'softmax')
teste <- neuralNetCV(x_train,y_train,model = model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = 'accuracy'
)
model %>% fit(
x_train,
y_train,
epochs = 200,
batch_size = 5,
validation_split = 0.2
)
dim(train)
x_train <- train[,1:14]
y_train <- train[,15]
model <- keras_model_sequential()
model %>%
layer_dense(units = 8, activation = 'relu', input_shape = c(15)) %>%
layer_dense(units = 3, activation = 'softmax')
model %>% compile(loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = 'accuracy')
model %>% fit(
x_train,
y_train,
epochs = 200,
batch_size = 5,
validation_split = 0.2
)
model <- keras_model_sequential()
model %>%
layer_dense(units = 8, activation = 'relu', input_shape = c(14)) %>%
layer_dense(units = 3, activation = 'softmax')
model %>% compile(loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = 'accuracy')
model %>% fit(
x_train,
y_train,
epochs = 200,
batch_size = 5,
validation_split = 0.2
)
model <- keras_model_sequential()
model %>%
layer_dense(units = 8, activation = 'relu', input_shape = c(14)) %>%
layer_dense(units = 1, activation = 'softmax')
model %>% compile(loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = 'accuracy')
model %>% fit(
x_train,
y_train,
epochs = 200,
batch_size = 5,
validation_split = 0.2
)
